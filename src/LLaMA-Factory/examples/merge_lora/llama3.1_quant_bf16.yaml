### Note: DO NOT use quantized model or quantization_bit when merging lora adapters

### model
model_name_or_path: models/llama3.1_70b_lora_sft
template: llama3

### export
export_dir: models/llama3.1_70b_lora_3bit_bf16_text
export_quantization_bit: 3
export_quantization_dataset: data/round1_text.json
export_quantization_maxlen: 256
export_quantization_nsamples: 1024
export_size: 4
export_device: cpu
export_legacy_format: true
infer_dtype: bfloat16
use_cache: false
print_param_status: true